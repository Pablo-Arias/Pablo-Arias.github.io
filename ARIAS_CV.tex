\documentclass[a4paper, 11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage[frenchb]{babel}
\usepackage{eurosym}
\usepackage{float}
\usepackage{longtable}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{hyperref}
\usepackage{soul}
\usepackage{tabto}
\usepackage{wrapfig}
\usepackage[margin=1in]{geometry}
\usepackage[super]{nth}
\usepackage[]{eurosym}
\usepackage{hyperref}




\sloppy
\addto\captionsfrench{% Replace "english" with the language you use
  \renewcommand{\contentsname}%
    {Table of contents}%
}

\newlength{\mytab} 
\setlength{\mytab}{1.8cm}

\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{Pablo Arias}
\chead{}
\rhead{Always up-to-date CV}


\begin{document}

%\begin{titlepage}
%\begin{center}
%\Huge{Curriculum Vitae}\\[1cm]
{\noindent \Huge{Pablo Arias Sarah}\\}
%\end{center}
%\vfill
%\tableofcontents
%\end{titlepage}
%\stepcounter{page}

\begin{wrapfigure}[5]{R}{0.15\textwidth}
\vspace{-1.5cm}
\hspace{-1.5cm}
\includegraphics[height=3.3cm]{pics/id_ircam3.png}
%Larnicol_4.jpg}
\end{wrapfigure}

\vspace{-0.2cm}
\noindent \begin{flushleft} Born on the \nth{28} of April 1990 (32 years old) \\
\noindent In Bogota, Colombia \\
\noindent Nationality : French, Colombian \\ \end{flushleft}

%\vskip 0.8cm

%Ma formation initiale est dans le domaine du traitement du signal sonore, de l'apprentissage machine et de l'intelligence artificielle. Après une période postdoctorale dans un laboratoire de neurosciences au Japon, j'ai étendu mes compétences dans le domaine de la psychologie et des neurosciences cognitives, puis de la neurophysiologie clinique. Mon HDR, soutenue en Nov. 2017, fait la synthèse de ce double parcours, en développant l'apport des sciences du signal sonore pour la recherche en cognition et les sciences du vivant. Je suis chargé de recherche CNRS depuis Octobre 2012 dans l'UMR9912 Sciences et Technologies de la Recherche et du Son (IRCAM/CNRS/Sorbonne Universités). 

\section{Carreer}
\vskip -0.8cm
\hspace{-0.3cm}\begin{longtable}{@{}p{1.3\mytab} p{.8\linewidth}}

\raggedright{Since Oct 2022}  & {\bf Postdoctoral Marie Curie Fellow} at the University of Glasgow, School of Neuroscience and Psychology, in collaboration with Lund University (Lund, Sweden). Resp. : Prof. Philippe Schyns \& Prof. Rachael Jack (University of Glasgow). \\[4pt]

\raggedright{2019--2022 \newline {\footnotesize 3 years, incl. 10 month sabbatical break}} & {\bf Postdoctoral Researcher} at Lund University Cognitive Science, Choice Blindness Lab, Department of Philosophy (Lund, Sweden), in collaboration with IRCAM (Paris, France). Resp. : Dr. Petter Johansson (Lund University; Swedish Collegium for Advanced Study). \\[4pt]

2019 \newline {\footnotesize7 months}  & {\bf Postdoctoral Researcher} at Sciences and technologies for music and sound (STMS UMR9912), IRCAM/CNRS/Sorbonne Université (Paris, France). Resp. : Dr. Jean-Julien Aucouturier (STMS).\\[4pt]

2015--2018 \newline {\footnotesize3 years}  & {\bf Assistant Researcher} at the Sciences and technologies for music and sound laboratory (STMS UMR9912), IRCAM/CNRS/SU (Paris, France). Resp. : Dr. Jean-Julien Aucouturier (CNRS).\\[4pt]

2015 \newline {\footnotesize6 months} & {\bf Research Engineer} at the Perception and Sound Design team (IRCAM/CNRS/Sorbonne Université), Paris, France. Resp. : Jean-Julien Aucouturier (CNRS).\\[4pt]

2014 \newline {\footnotesize6 months} & {\bf Research Internship} at IRCAM/CNRS/Sorbonne Université (Paris, France). Resp. : Frédéric Bevilacqua (IRCAM), Norbert Schnell (IRCAM), Jules Françoise (IRCAM).\\[4pt]

2013 \newline {\footnotesize6 months} & {\bf Computer Science Engineer} at Flux: Sound and picture and Development (Orléans, France). Resp. : Gael Martinet (Flux).\\[4pt]

Before 2013 : & Research project at Polytech Nantes (music neuroscience, Nantes, France) ; Research Internship at BK Birla Institute Of Engineering and Technology (embeded hardware development, Pilani, India); Internship at Eowave (analog instrument development, Paris, France); Research Project at Polytech Nantes (Midi controler development); Research Project with G.E. Healthcare (signal processing). \\

\end{longtable}

\section{Education}
\vskip -0.8cm
\hspace{-0.3cm}\begin{longtable}{@{}p{1.3\mytab} p{.8\linewidth}}
2015--2018 : & {\bf PhD in Cognitive Science}, from Sorbonne Universités (Paris, France). Title : \emph{``The cognition of auditory smiles : a computational approach''}. Dir. : Jean-Julien Aucouturier (IRCAM/CNRS/SU), Pascal Belin (CNRS/Aix Marseille Univ.), Patrick Susini (IRCAM/CNRS/UPMC). Jury : Rachael Jack (Univ. Glasgow), Tecumseh Fitch (Uiv. Vienna), Julie Grèzes (ENS), Catherine Pelachaud (ISIR), Martine Gavaret (Univ. Paris Descartes). Doctoral school : Cerveau, Cognition, Comportement (ED3C). \\[4pt]

2013--2014 : & {\bf Master of Science}, Acoustics, signal processing and computer science applied to music. Sorbonne Universités (Paris, France).\\[4pt]

2010--2013 : & {\bf Master of Engineering}, Engineer in Electronics and digital technologies specialized in multimedia. Polytech' Nantes (Nantes, France).\\[4pt]

2008--2010 : & {\bf Two year Preparatory University Degree}, Mathematics \& computer science. Institute of sciences and techniques. Nantes University (Nantes, France).\\[4pt]

1994--2008 : & {\bf Elementary \& Secondary School}, Scientific Baccalaur\'{e}at specialized in mathematics with awards (French national diploma). ICFES (Colombian national diploma). Lyc\'{e}e Français Louis Pasteur (Bogota, Colombia).\\
\end{longtable}

\subsection*{Online courses and certifications}
\hspace{-0.3cm}\begin{longtable}{@{}p{1.3\mytab} p{.8\linewidth}}
2015--2016 : & {\bf Principles of fMRI 1 and 2}, Coursera course organised by Johns Hopkins University. (online course) \\[4pt]
2014--2015 : & {\bf Neuroanatomy certification}, obtained from Neurocourses, King's College University of London (Londres, UK). \\[4pt]
\end{longtable}

\section{Publications}
\subsection*{In prep}

\begin{enumerate}[1.]

\item \ul{Arias Sarah, P.}, Bedoya, D., Aucouturier, J-J., Hall, L. \& Johansson, P. \emph{Artificially aligning  the smiles of speed-dating participants increases liking and rapport.}. (in prep)

\item \ul{Arias Sarah, P.}, Denis G., Aucouturier J-J., Hall L., Schyns P., Jack, R., \& Johansson P. \emph{DuckSoup: An open-source experimental platform to  manipulate facial and vocal signals during social interactions}. (in prep)	

\end{enumerate}


\subsection*{In review, in press}

\begin{enumerate}[1.]
\item Nakai, T., Rachman, L., \ul{Arias Sarah, P.}, Okanoya, K., \& Aucouturier, J. J. (2023). \emph{A language-familiarity effect on the recognition of computer-transformed vocal emotional cues}. bioRxiv, 521641. (in review) {\footnotesize \href{https://www.biorxiv.org/content/biorxiv/early/2019/01/17/521641.full.pdf}{[preprint]} }

\item Rodriguez H, \ul{Arias Sarah P.}, Cannone C (2023) \emph{Contrasts of register underlie the perception of musical humor}. (in press)
\end{enumerate}


\subsection*{Peer-reviewed}
\begin{enumerate}[1.]
\item \ul{Arias Sarah, P.}, Hall, L., Saitovitch, A., Aucouturier, J-J., Zilbovicus, M. \& Petter Johansson  (2023) \emph{Pupil dilation reflects the dynamic integration of audiovisual emotional speech}. Scientific Reports, 13, 5507 {\footnotesize \href{https://www.nature.com/articles/s41598-023-32133-2}{[open access]}}

\item Rosi, V., \ul{Arias Sarah, P.}, Houix, O., Misdariis, N., \& Susini, P. (2023). \emph{Shared mental representations underlie metaphorical sound concepts}. Scientific Reports, 13(1), 5180. {\footnotesize \href{https://www.researchgate.net/profile/Victor-Rosi/publication/365198614_Shared_Mental_Representations_Underlie_Metaphorical_Sound_Concepts/links/636a3a5954eb5f547cb30de1/Shared-Mental-Representations-Underlie-Metaphorical-Sound-Concepts.pdf}{[open access]} }


\item Salais, L., \ul{Arias, P.}, Le Moine, C., Rosi, V., Teytaut, Y., Obin, N., \& Roebel, A., (2022) \emph{Production Strategies of Vocal Attitudes}. In Interspeech 2022 (pp. 4985-4989). ISCA. {\footnotesize  \href{https://www.researchgate.net/profile/Clement-Le-Moine/publication/359982566_Production_Strategies_of_Vocal_Attitudes/links/62599cefa279ec5dd7f91c3f/Production-Strategies-of-Vocal-Attitudes.pdf}{[open access]}}


\item Bedoya, D., \ul{Arias, P. }, Rachman, L. , Liuni, M., Canonne, C. , Goupil, L. \& J-J. Aucouturier, (2021) \emph{Even a violin can cry: Expressive cues of specific vocal origin drive the perception of emotions in both vocal and non-vocal music}. Philosophical Transactions of the Royal Society B, 376(1840), 20200396. {\footnotesize \href{https://royalsocietypublishing.org/doi/pdf/10.1098/rstb.2020.0396}{[open access]}}

\item \ul{Arias, P.}, Bellmann, C., Aucouturier, \& Aucouturier, JJ. (2021) \emph{Facial mimicry in the congenitally blind}. Current Biology, 31(19), R1112-R1114. {\footnotesize \href{https://hal.archives-ouvertes.fr/hal-03391957/document}{[open access]}}

\item \ul{Arias, P.}, Rachman, L., Liuni, M. \& Aucouturier, JJ. (2020) \emph{Beyond correlation: voice-transformation methods for the experimental study of emotional speech.} Emotion Review. {\footnotesize \href{https://hal.archives-ouvertes.fr/hal-02907502/document}{[open access]}}

\item Wollman, I., \ul{Arias, P.}, Aucouturier, J-J., \& Morillon, B. (2019). \emph{Neural entrainment to music is sensitive to melodic spectral complexity}. Journal of Neurphysiology. {\footnotesize \href{https://hal.archives-ouvertes.fr/hal-02475989/document}{[open access]}}


\item \ul{Arias, P.}, Belin, P. \& Aucouturier, JJ. (2018) \emph{Auditory smiles trigger unconscious facial imitation}. Current Biology, vol.28(14), R782-R783.{\footnotesize \href{https ://www.cell.com/current-biology/fulltext/S0960-9822(18)30752-8}{[open access]}}

\item Ponsot, E., \ul{Arias, P.} \& Aucouturier, J-J. (2018) \emph{Uncovering mental representations of smiled speech using reverse correlation
}. The Journal of the Acoustical Society of America, vol. 143 (1), EL19--EL24. {\footnotesize \href{https://asa.scitation.org/doi/abs/10.1121/1.5020989}{[open access]}}

\item \ul{Arias, P.}, Soladie, C., Bouafif, O., Robel, A., Seguier, R., \& Aucouturier, J. J. (2018). Realistic transformation of facial and vocal smiles in real-time audiovisual streams. IEEE Transactions on Affective Computing. {\footnotesize \href{https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8307228}{[open access]}}

\item Rachman, L., Liuni, M., \ul{Arias, P.}, Lind, A., Johansson, P., Hall, L., ... \& Aucouturier, J. J. (2018). DAVID: An open-source platform for real-time transformation of infra-segmental emotional cues in running speech. Behavior research methods, 50(1), 323-343. \\{\footnotesize \href{https://link.springer.com/article/10.3758/s13428-017-0873-y}{[open access]}}

\end{enumerate}
\vskip 6pt 
\noindent My complete list of publications can be found online \href{https://scholar.google.fr/citations?user=6jMFwJQAAAAJ&hl=en&oi=ao}{here}. 

\section{Grants}
\vskip -0.8cm
\hspace{-0.3cm}
\begin{longtable}{@{}p{1.3\mytab} p{.8\linewidth}}

2022  & DuckSoup: An open-source experimental platform to manipulate participants’ facial and vocal attributes during social interactions. Swedish Research Council. Co-awarded to Lars Hall \& Petter Johansson (Lund University Cognitive Science, Lund, Sweden); 450k\euro \\[4pt]

2022  &  Sound Environment Infrastructure Grant. Co-awarded to Petter Johansson (Lund University Cognitive Science, Lund, Sweden); 5k\euro \\[4pt]

2022  & Marie Skłodowska-Curie postdoctoral Fellowship. SINA (Studying Social Interactions with Audiovisual Transformations). w/ Philippe Schyns \& Rachael E. Jack (University of Glasgow) and Petter Johansson; 190k\pounds \\[4pt]

2021  & Sorbonne Université Emergence. Revolt (Revealing Human Bias with Real Time Vocal Deep Fakes). Co-awarded to Nicolas Obin. 60k\euro \\[4pt]

2019  & Postdoctoral position received in an open international competition by Lund University; The Crafoord Foundation \\[4pt]
\end{longtable}

\section{Scientific Communication}
\subsection*{Patents}
\begin{enumerate}
\item  \ul{Arias, P.}, Aucouturier, J-J. \& Roebel., A., Method and apparatus for dynamic modifying of the timbre of the voice by frequency shift of the formants of a spectral envelope. (2017), (EP2018/053433). 
\end{enumerate}

\subsection*{Theses}
\begin{enumerate}	
\item \ul{Arias, P.}, (2018). The cognition of auditory smiles : a computational approach. PhD Thesis. {\footnotesize \href{https://hal.archives-ouvertes.fr/tel-02010161/file/PhD%20Arias.pdf}{[open access]}}



\item \ul{Arias, P.}, Fran\c{c}oise, J., Bevilacqua, F., \& Schnell, N. (2014). Sound description and synthesis in the MaD (Mapping by demonstration) framework. M. Sc. Thesis.
\end{enumerate}

\subsection*{Peer Reviewed Abstracts}
\begin{enumerate}
	\item \ul{Arias, P.}, Bedoya, D., Johansson, P., Hall, L.\& Aucouturier, J-J., Controlling dyadic interactions with real-time smile transformations. Society for Affective Science Conference, worldwide/online, April 2021.
	\item \ul{Arias, P.}, Belin, P., \& Aucouturier, J.-J., Hearing smiles and smiling back. Laughter workshop. Institut des Systèmes Intelligents et de Robotique (ISIR). Paris, France, September 2018.
\end{enumerate}

\subsection*{Conferences}
\begin{enumerate}
	\item Nölle, J., Wu, Y., \ul{Arias, P.}, Garrod, O. G. B., Schyns, P. G. \& Jack, R. E. (2022). More Than Affect: Human Facial Expressions Provide Iconic and Pragmatic Functions. In Proceedings of the Joint Conference on Language Evolution (JCoLE). {\footnotesize \href{https://evolang.org/jcole2022/proceedings/papers/JCoLE2022_paper_262.pdf}{[open access]}}

	\item Rodriguez, H., \ul{Arias, P.}, \& Canonne, C. (2021). Investigating the pragmatic effects of musical syntax through musical humor. In 15th International Symposium of Cognition, Logic, and Communication.

	\item \ul{Arias, P.}, Influencing romantic decisions with real time smile transformations, Virtual Social Interaction Conference, online, University of Glasgow, June 2021.

	\item \ul{Arias, P.}, Computational techniques to synthesize emotional stimuli, Society for Affective Science Conference, worldwide/online, April 2021.

	\item \ul{Arias, P.}, Bedoya, D., Johansson, P., Hall, L. \& Aucouturier, J-J., Controlling dyadic interactions with real-time smile transformations. Society for Affective Science Conference, Online conference, April 2021.
	
	\item \ul{Arias, P.}, Hearing facial expressions of emotion, the case of the smile, cSCAN, Glasgow, Scotland, April 2019.

	\item \ul{Arias, P.}, Auditory smiles trigger unconscious facial reactions. Contextual: How the Social Context Shapes Brain and Behaviour. International conference of the European Society for Cognitive and Affective Neurosciences (ESCAN). Leiden, NL, July 2018. 

	\item \ul{Arias, P.}, Belin, P. , \& Aucouturier, J-J., Unconsciously imitating smiles heard in speech \textemdash and hearing smiles in musical sounds. European Research Music Conference. Barcelona, Spain, June 2018. 

	\item \ul{Arias, P.}, Ziggy : the rise and fall of zygomatic muscles in speech. Conference of the Consortium of European Research on Emotions (CERE), Glasgow, United Kindgom, April 2018.

	\item \ul{Arias, P.}, Repr\'esentations mentales du sourire dans la voix parl\'e: une \'etude par corr\'elation inverse. Congrès Français d'Acoustique (CFA), Le Havre, France, Mars 2018.	 


	\item \ul{Arias, P.}, Spectral cues caused by smiling trigger unconscious facial imitation. Music Language and Cognition Summer School. Como, Italy, June 2017.

	\item \ul{Arias, P.}, Emotional mimicry induced by manipulated speech. Workshop on Music cognition, emotion and audio technology in Tokyo. University of Tokyo, Tokyo, Japan, November 2016.

	\item \ul{Arias, P.}, Emotional mimicry induced by manipulated speech. Journ\'ees Jeunes Chercheurs en Audition Acoustique musicale et Signal audio (JJCAAS). France, Paris, November 2016.

	\item \ul{Arias, P.}, Time perception and neural oscillations modulated by speech rate. Festival IRCAM-Manifeste, Paris, France, October 2016.

	\item Rachman, L., Liuni, M., \ul{Arias, P.}, \& Aucouturier, J. J., Synthesizing speech-like emotional expression onto music and speech signals. In Fifth International Conference on Music and Emotions. ICME4, Geneva, CH, October 2015.
\end{enumerate}

\subsection*{Invited Talks}
\begin{enumerate}
	\item \ul{Arias, P.}, Studying social interactions with voice and face transformations, Mosaic grand challenge, online, October 2022
	\item \ul{Arias, P.}, Hacking social interactions with real time voice/face transformations, Karolinska Institutet, Stockholm, September 2021
	\item \ul{Arias, P.}, Hacking social interactions with real time voice/face transformations, Universidad de El bosque, Bogota, Colombia, August 2021
	\item Studying social interactions with real time voice/face transformations, IRCAM, Paris, France. July 2021.
	\item \ul{Arias, P.}, Manipulating speed dating interactions with real time smile transformations. Equipe Neuro-Ethologie Sensorielle. Saint-Etienne, France. June 2021
	\item \ul{Arias, P.}, Manipulating speed dating interactions with real time smile transformations. Social Cognitive Neuroscience Lab, Hebrew University of Jerusalem, Israel. June 2021. 
	\item \ul{Arias, P.}, Beyond correlation: voice-transformation methods for the experimental study of voice perception. Lund University cognitive science, Lund, Sweden. February 2020
	\item \ul{Arias, P.}, Entendre les émotions dans la voix de l'autre. Présentation Invité. INJA, Paris, France, November 2019.
	\item \ul{Arias, P.}, Intégration audiovisuelle des sourires, une étude eye tracking, 5ième Journée du club eye tracking et autisme, Tours, France, Mars 2019	
	\item \ul{Arias, P.} Hearing Facial Expressions of Emotion, the case of the smile. Inst Neurosciences des Systèmes. Marseille, France. May 2019
	\item \ul{Arias, P.}, Unconscious physiological reactions to smiles in speech revealed by super-vp’s frequency warping. Journée RIM. IRCAM, Paris, France, October 2017.
	\item \ul{Arias, P.} Spectral cues caused by smiling trigger unconscious facial imitation, Language Music and Cognition Summer School, Lake Como School of Advanced Studies, 24-26 June 2017, Como, Italy
	\item \ul{Arias, P.}, Unconscious reactions to auditory smile. ISIR. Paris, France. May 2017.


\end{enumerate}	

\section{Teaching and management}

\subsection*{Guest lectures}
\vskip -0.8cm
\hspace{-0.3cm}\begin{longtable}{@{}p{1.3\mytab} p{.8\linewidth}}
2021 : & Centrale Supelec (Rennes, France). Master Course, ``Voice and face processing technology for the study of unconscious human behavior``, total : 1h.\\[4pt]
2019 : &  New York University (NYU). Master Course, ``Essential Aspects of Voice Perception'', total : 3h.\\[4pt]
2018--2019 : &  Ircam Composition Cursus. Master Course. ``Essential Aspects of Voice Perception'', 2 x 2 hours, total : 4h.
\end{longtable}

\subsection*{PhD external examining}
\begin{description}
\item Andrey Anikin, \emph{Human non-verbal vocalizations}. Cognitive Science, Lund University (2020).
\end{description}


\subsection*{Workshops}
\begin{enumerate}	
	\item Arias, P., Trois outils de traitement de la voix \'emotionnelle et leurs effets physiologiques. École nationale d'ingénieurs de Tunis (ENIT), Journ\'ee d'\'etudes : TICs, Musique et \'emotion. Tunisie, Tunis, Janvier 2018.
	\item Arias, P., Liuni M., Transformations émotionnelles de la voix parlée \textemdash conséquences comportementales et physiologiques. Journée voix - Studio 5 en direct, IRCAM, Paris, October 2017.
\end{enumerate}

\subsection*{Management and supervision}
\begin{description}
\item Guillaume Denis (software engineer, 2020-2022), Sarah Vlasitz (Master Student, 2022, in collaboration with Tecumseh Fitch and Friederike Range, University of Vienna, Austria), Daniel Bedoya (Research Assistant, 2019), Lou Seropian (Research Assistant, 2018), Louise Vasa (Research Assistant, 2017), Mael Garnotel (Research Assistant, 2016).
\end{description}


\section{Scientific Contributions}

\subsection*{Conference Organisation}
\vskip -0.8cm
\hspace{-0.3cm}
\begin{longtable}{@{}p{1.3\mytab} p{.8\linewidth}}

2022  & Smila workshop, ``Smiling and Laughter across contexts and the life-span``, Program Committee, 2022 \\
2022  & ``Affective Computing and Intelligent Interaction 2022'' (ACII), Workshop Chair, Nara, Japan, 18-21 October, 2022; \\
2016  & ``Journées Jeunes Chercheurs en Audition, Acoustique Musicale et Signal audio (JJCAAS)'', Paris, 23, 24, 25 November 2016;
\end{longtable}


\subsection*{Reviewer}
\begin{description}
\item ACM Transactions on Interactive Intelligent Systems (2022), Nature Scientific Reports (2021), Royal Society Open Science (2021), Emotion (2021), Journal of Nonverbal Behavior (2021); Psychonomic Bulletin \& Review (2021), Frontiers Digital Health (2020); Language Resources and Evaluation (2019); Journal of Experimental Social Psychology (JESP) (2019); Music\ae\ Scienti\ae\ (2016);
\end{description}

\section{Scientific Dissemination}
\subsection*{Software packages and resources}
\vskip -0.8cm
\hspace{-0.3cm}\begin{longtable}{@{}p{1.3\mytab} p{.8\linewidth}}

Mozza : & A gstreamer plugin to transform facial smiles in real time. Coming in open source format in 2023. Kermarec L., \ul{Arias Sarah P.}, Aucouturier J-J. \\[4pt]

DuckSoup : & A videoconference experimental platform to perform social interaction experiments while manipulating participants voice/face in real time. Denis, G., \ul{Arias Sarah} P., Aucouturier J-J, Hall, L., Johansson P., {\footnotesize \href{https://github.com/ducksouplab/ducksoup}{[open source]}} \\[4pt] 

STIM :  & A python toolbox to process and analyse experimental video and audio stimuli. Arias Sarah, P. {\footnotesize \href{https://github.com/Pablo-Arias/STIM}{[open source]}}  \\[4pt]

Ziggy : & A real time algorithm to parametrically control smiles in speech. Arias Sarah, P. {\footnotesize \href{https://github.com/Pablo-Arias/STIM}{[open source]}} \\

NCSE : & Neutral content said emotionally voice dataset {\footnotesize \href{https://cream.ircam.fr/?p=120}{[open data]}}

\end{longtable}
\noindent Open source code: {\scriptsize \url{https://github.com/Pablo-Arias/STIM}}

\subsection*{Multimedia content}
\vskip -0.8cm
\hspace{-0.3cm}\begin{longtable}{@{}p{1.3\mytab} p{.8\linewidth}}
Documentary :  & ``Craquer le code émotionnel de la musique'' (réal. Mailys Audouze, 2014-2021; currently in festivals) {\footnotesize \url{https://vimeo.com/185950491}} \\[4pt]
Podcasts : & ``Perception of Smiles in the Voice.'' The Voice Tech Podcast (2018) \newline ``Souriez vous-êtes écoutés : le son des émotions'', Savant Sachant Chercher, Science Infuse (2019)
\end{longtable}

\subsection*{Scientific events for the general public}
\begin{description}
\item  \emph{Sound and Behavior} roundtable, the week of sound, UNESCO (Jan. 2021)  ; portes ouvertes IRCAM (2015-2019) ; journée voix IRCAM (2017) ; Science Agora Tokyo (Nov. 2016)  ; Ircam Forum (2016);
\end{description}

\subsection*{General Media}
\begin{description}
\item French media : Le Monde, Science et Avenir, Valeurs Actuelles, Konbini France
\item International media  : Cosmos Magazine, Discover, Quartz, Vrt nws, NPO Radio 1
\end{description}
Article {\footnotesize \href{https://qz.com/1342753/smiles-have-a-sound-and-its-contagious-a-study-says/}{[example]}}

\section{Interests \& skills}
\vskip -0.8cm
\hspace{-0.3cm}\begin{longtable}{@{}p{1.3\mytab} p{.8\linewidth}}
Current interests :& Social interactions, social dynamics, romantic attraction, human emotions, imitation, social alignment, unconscious emotion processing, affective reactions, vocal and facial expressions of emotion, affective computing, voice and face transformations, artificial intelligence. \\[4pt]

Measures : & Electromyography, eye tracking, pupillometry, electrodermal activity, electroencephalography, sensorless physiology (computer-vision based face analysis), voice and speech analysis, behavior. \\

Experimental paradigms : & auditory reverse correlation, facial mimicry, speed dating, decision making, free social interactions, stimulus response. \\

Data analysis \& statistics: & Data wrangling, Generalised linear mixed models (GLMMs), permutation tests, time series analysis (e.g, cross-correlation, Transfer Response Functions, Mutual Information), causal mediation, social relation models \\

Programing :&  Python (polars, pandas, numpy, conda, ...), R (lmer, mediation, ...), Docker, Server development, git, HTML, C++. \\

Speech \& \newline sound :& Digital signal processing, acoustics, sound analysis, sound design, Max, Ableton Live, Praat, mixing, mastering, music theory, piano, guitar, OSC, MIDI. \\

Personal :& Cooking (vegetarian, french, indian and colombian traditional recipes), Composition and music production. Hiking (autonomous, solo, multiday). Meditation (10 day silent meditation retreat in 2020), biking, jogging, chess, yoga, critical listening. \\

\end{longtable}
 
\end{document}